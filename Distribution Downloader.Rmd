---
title: "Untitled"
author: "Chris Talbot"
date: "11/13/2022"
output:
  html_document: default
  pdf_document: default
---

```{r}
library(jsonlite)
library(raster)
library(maps)
library(maptools)
library(rangemap)
library(dismo)
library(rgbif)              #for downloading datasets from gbif
library(countrycode)        #for getting country names based on countryCode important
library(rnaturalearth)      #for downloading maps
library(sf)                 #for manipulating downloaded maps
library(tidyverse)          #for tidy analysis
library(CoordinateCleaner)  #for quality checking of occurrence data
library(ggplot2)
library(PBSmapping)
```

```{r}
# This one downloads range data

starting_point = 1

# Get the list of species to evaluate
import_filename = "flowers.csv"
species_list = data.frame(read.csv(import_filename))

for(species_num in starting_point:nrow(species_list)) {
  # Get the species taxon key
  species_name_upd = species_list[species_num, "taxon"]
  species_name_old = species_list[species_num, "species"]
  species_shp_file_upd = paste(species_name_upd, "shp", sep=".")
  species_shp_file_old = paste(species_name_old, "shp", sep=".")
  if(file.exists(species_shp_file_upd) 
     || file.exists(species_shp_file_old)) { next }
  
  success = TRUE
  usage_key = name_backbone(name = species_name_upd, rank = "species") 
  usage_key = tryCatch(
      {
        pull(usage_key, usageKey)
      },
      error=function(cond) { return(NA) 
        print(species_name) },
      warning=function(cond) { return(NA)
        print(species_name) }
    )  
    
  if(is.na(usage_key)) { next }

  
  # Get a download key using the taxon key
  gbif_download_key = occ_download(pred("taxonKey", usage_key),
                             format = "SIMPLE_CSV",
                             user = "chtalbot",
                             pwd = "Cathamster6!",
                             email = "chtalbot@umich.edu")

  # Download a CSV of species occurrences with the download key
  occ_download_wait(gbif_download_key)

  species_download_file = paste(species_name_upd, "zip", sep=".")
  gbif_download = occ_download_get(gbif_download_key,
                                   path = species_download_file,
                                   overwrite = TRUE) %>%
                    occ_download_import() %>%
                    setNames(tolower(names(.)))
  
  if(nrow(gbif_download) >= 1000000) { next }
    
  # Remove fossils and living specimens
  cleaned_data = gbif_download %>%
    as_tibble() %>%
    filter(!basisofrecord %in% c("FOSSIL_SPECIMEN",
                                 "LIVING_SPECIMEN"))
  print(paste0(nrow(gbif_download)-nrow(clean_step1), " records deleted; ",
               nrow(clean_step1), " records remaining."))
  
  # Clean the coordinates a lot
  cleaned_data = cleaned_data %>%
    # Remove NA coordinates
    filter(!is.na(decimallatitude),
           !is.na(decimallongitude)) %>%
    cc_dupl() %>%
    cc_zero() %>%
    cc_equ() %>%
    cc_val() %>%
    cc_sea() %>%
    cc_cap(buffer = 2000) %>%
    cc_cen(buffer = 2000) %>%
    cc_gbif(buffer = 2000) %>%
    cc_inst(buffer = 2000)
  print(paste0(nrow(gbif_download)-nrow(clean_step2), " records deleted; ",
               nrow(clean_step2), " records remaining."))
  
  #  Remove occurrences with highly uncertain coordinates
  cleaned_data = cleaned_data %>%
   filter(is.na(coordinateuncertaintyinmeters) |
            coordinateuncertaintyinmeters < 10000,
         is.na(coordinateprecision) |
            coordinateprecision > 0.01)
  print(paste0(nrow(gbif_download)-nrow(clean_step3), " records deleted; ",
               nrow(clean_step3), " records remaining." ))

  # Remove occurrences from before 1960
  cleaned_data = cleaned_data %>% filter(year >= 1960)
  print(paste0(nrow(gbif_download)-nrow(clean_step3), " records deleted; ",
               nrow(cleaned_data), " records remaining." ))
  
  # Create a dataframe of the species occurrence coordinates
  lons = cleaned_data["decimallongitude"]
  lats = cleaned_data["decimallatitude"]
  lon_lat = data.matrix(data.frame(c(lons, lats)))

  occurrences = data.frame(species = species_name, 
                                     lon = lons, lat = lats)
  
  rangemap_buffer(occurrences, 10000, save_shp=TRUE, name = species_name)
}
```


```{r}
library(dplyr)
library(rgbif) 

# Get the list of species to evaluate
import_filename = "data/flowers_master.csv"
species_list = data.frame(read.csv(import_filename))
species_names = species_list[,"taxon"]
list1 = species_names[1:365]
list2 = species_names[(365+1):(365*2)]
list3 = species_names[((365*2)+1):((365*3)+1)]

# Get the taxon keys for the species list
gbif_taxon_keys_1 <- list1 %>% 
name_backbone_checklist() %>% # match to backbone 
filter(!matchType == "NONE") %>% # get matched names
pull(usageKey) 

gbif_taxon_keys_2 <- list2 %>% 
name_backbone_checklist() %>% # match to backbone 
filter(!matchType == "NONE") %>% # get matched names
pull(usageKey) 

gbif_taxon_keys_3 <- list3 %>% 
name_backbone_checklist() %>% # match to backbone 
filter(!matchType == "NONE") %>% # get matched names
pull(usageKey) 

```
```{r}
# download the data
occ_download (
  pred_in("taxonKey", gbif_taxon_keys_5), 
  pred("HAS_GEOSPATIAL_ISSUE", FALSE),
  pred("HAS_COORDINATE",TRUE),
  pred("OCCURRENCE_STATUS","PRESENT"),
  pred_not(pred_in("BASIS_OF_RECORD",
    c("FOSSIL_SPECIMEN","LIVING_SPECIMEN"))),
  pred_gte("YEAR", 1960),
  pred_or(pred_lte("COORDINATE_UNCERTAINTY_IN_METERS", 10000), 
          pred_isnull("COORDINATE_UNCERTAINTY_IN_METERS")),
  pred_not(pred_in("COORDINATE_UNCERTAINTY_IN_METERS", c(301, 3036, 999, 9999))),
  pred_not(pred("DECIMAL_LATITUDE", 0)),
  pred_not(pred("DECIMAL_LONGITUDE", 0)),
  pred_gte("distanceFromCentroidInMeters", 2000),
  pred_in("continent", c("NORTH_AMERICA", "SOUTH_AMERICA")),
  format = "SIMPLE_CSV"
)
```


```{r}
library(dplyr)
library(rgbif) 
library(data.table)

# TO DO: Import species data
#        Loop through each species
#        Do a few more cleaning tasks (e.g. CoordinateCleaner functions)
#        Make a data.frame with [species_name, lons, lats]
#        Create and export polygons representing distribution (buffered points or alpha-shapes?)

data = fread("occurrences/occ_1.csv")
data = data[, .(species, decimalLatitude, decimalLongitude)]
write.csv(data, "occ_1_trimmed.csv")
data = fread("ranges/occ_2.csv")
data = data[, .(species, decimalLatitude, decimalLongitude)]
write.csv(data, "occ_2_trimmed.csv")
data = fread("ranges/occ_3.csv")
data = data[, .(species, decimalLatitude, decimalLongitude)]
write.csv(data, "occ_3_trimmed.csv")
data = fread("ranges/occ_4.csv")
data = data[, .(species, decimalLatitude, decimalLongitude)]
write.csv(data, "occ_4_trimmed.csv")
data = fread("ranges/occ_5.csv")
data = data[, .(species, decimalLatitude, decimalLongitude)]
write.csv(data, "occ_5_trimmed.csv")
```
